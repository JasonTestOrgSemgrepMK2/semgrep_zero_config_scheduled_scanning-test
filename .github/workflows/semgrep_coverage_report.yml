name: Semgrep Coverage Report

on:
  # Scan on-demand through GitHub Actions interface:
  workflow_dispatch: {}
  # Schedule the CI job (this method uses cron syntax):
  schedule:
    - cron: '20 17 * * *' # Sets Semgrep to scan every day at 17:20 UTC.

jobs:
  get-list-of-repos:
    runs-on: ubuntu-latest

    steps:
    - name: Checkout
      uses: actions/checkout@v3

    - name: Set up Python
      uses: actions/setup-python@v4
      with:
        python-version: '3.9'

    - name: Install dependencies
      run: |
        pip install requests
        pip install pandas
        
    - name: Get list of repositories
      env:
        # Generate PAT with Read-Only access to all repos in your GH ORG
        PAT_READ_ONLY_CUSTOMER_REPO: ${{ secrets.PAT_READ_ONLY_CUSTOMER_REPO }}

      run: |
        import requests
        import os
        import requests
        import logging
        import time
        import pandas as pd
      
        # Set up logging
        logger = logging.getLogger()
        logger.setLevel(logging.INFO)

        # GitHub repository in the format "org_name/repo_name"
        full_repo_name = os.environ['GITHUB_REPOSITORY']
        # Extract the organization name
        org_name = full_repo_name.split('/')[0]
        print("Organization Name:", org_name)
        pat = os.environ['PAT_READ_ONLY_CUSTOMER_REPO']
        headers = {'Authorization': f'token {pat}'}

        # response = requests.get(f'https://api.github.com/orgs/{org_name}/repos', headers=headers)
        # repos = response.json()

        # Initialize empty list to hold all repositories
        all_repos = []
        
        # Initial URL to fetch repositories
        url = f'https://api.github.com/orgs/{org_name}/repos'
        
        # Loop through all pages of results
        while url:
            response = requests.get(url, headers=headers)
            # Check if the request was successful
            if response.status_code == 200:
                repos = response.json()
                all_repos.extend(repos)
                
                # GitHub provides the link to the next page in the response headers
                if 'next' in response.links.keys():
                    url = response.links['next']['url']
                else:
                    url = None  # No more pages
            else:
                logging.error(f"Failed to fetch repositories: {response.status_code}")
                break

        # Read the CSV files
        daily_df = pd.read_csv('daily.csv', header=None)
        weekly_df = pd.read_csv('weekly.csv', header=None)
        
        # Convert to lists for easier checking
        daily_repos = daily_df[0].tolist()
        weekly_repos = weekly_df[0].tolist()
        
        # Prepare the data for the new CSV file
        coverage_data = []
        
        for repo in repos:
            repo_name = repo['name']
            created_at = repo['created_at']
            html_url = repo['html_url']
            daily_mark = 1 if repo_name in daily_repos else 0
            weekly_mark = 1 if repo_name in weekly_repos else 0
            not_covered = 1 if (daily_mark+weekly_mark == 0) else 0
            duplicate_scans = 1 if (daily_mark+weekly_mark == 2) else 0            
            coverage_data.append([repo_name, daily_mark, weekly_mark, not_covered, duplicate_scans, created_at, html_url])
        
        # Create a DataFrame and write to CSV
        coverage_df = pd.DataFrame(coverage_data, columns=['Repository', 'Daily', 'Weekly', 'Not Covered', 'Duplicate Scans', 'Created At', 'URL'])
        coverage_df.to_csv('coverage.csv', index=False)
      shell: python
      
    - name: Upload coverage.CSV as Artifact
      uses: actions/upload-artifact@v2
      with:
        name: semgrep-coverage-report.csv
        path: coverage.csv  # Replace with the path to your CSV file
